{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def auto_correlation(data):\n",
    "    \"\"\"\n",
    "    Calculate the auto-correlation function for a given data array.\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    variance = np.var(data) # Calculates the variance of a given data array\n",
    "    mean = np.mean(data)\n",
    "    autocorr = np.correlate(data - mean, data - mean, mode='full') # Mode='full' parameter ensures that the correlation is computed for all possible time lags. \n",
    "    autocorr = autocorr[n-1:] / (variance * np.arange(n, 0, -1))\n",
    "    return autocorr\n",
    "\n",
    "# Load data from CSV file\n",
    "data = pd.read_csv(\"/Users/ankitgupta/Desktop/1um_1by1000/5_DLS3.csv\")\n",
    "\n",
    "# Extract the intensity data from the third column\n",
    "intensity = data.iloc[:,0].values\n",
    "\n",
    "# Calculate the auto-correlation function\n",
    "autocorr1 = auto_correlation(intensity)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "data1 = pd.read_csv(\"/Users/ankitgupta/Desktop/DLS/DLS_exp/3 micro/out_3udls1.csv\")\n",
    "data2 = pd.read_csv(\"/Users/ankitgupta/Desktop/DLS/DLS_exp/3 micro/out_3udls2.csv\")\n",
    "data3 = pd.read_csv(\"/Users/ankitgupta/Desktop/DLS/DLS_exp/3 micro/out_3udls3.csv\")\n",
    "\n",
    "\n",
    "intensity1 = data1.iloc[2:,1].values\n",
    "intensity2 = data2.iloc[2:,1].values\n",
    "intensity3 = data3.iloc[2:,1].values\n",
    "\n",
    "\n",
    "print(len(intensity1), len(intensity2), len(intensity3))\n",
    "\n",
    "\n",
    "avg = (intensity1 + intensity2 + intensity3 )/3\n",
    "avg = avg[10:21000]\n",
    "\n",
    "xdata = np.arange(0,len(avg))\n",
    "xdata = xdata/21000\n",
    "\n",
    "\n",
    "# Plot the auto-correlation function\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(xdata,avg)\n",
    "plt.xlabel('Time lag')\n",
    "plt.ylabel('Auto-correlation')\n",
    "plt.xscale(\"log\")\n",
    "#plt.yscale(\"log\")\n",
    "#plt.savefig(\"/Users/ankitgupta/Desktop/autocorrelation.png\")\n",
    "plt.show()\n",
    "\n",
    "mytable2 = pd.DataFrame({\"Time\" : xdata,\n",
    "    \"intensity \" : avg, })\n",
    "\n",
    "# Convert table to .csv format\n",
    "mytable2.to_csv(\"/Users/ankitgupta/Desktop/DLS/DLS_exp/3 micro/final_autocorr.csv\")\n",
    "\n",
    "print(len(avg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Read data from CSV file\n",
    "data = pd.read_csv(\"/Users/ankitgupta/Desktop/DLS/DLS_exp/1 micro/final_autocorr.csv\", header=None).values\n",
    "\n",
    "# Split the data into time and autocorrelation arrays\n",
    "tau = data[:,1]\n",
    "time = tau \n",
    "autocorr = data[:,2]\n",
    "\n",
    "# Fitting function\n",
    "def func(x,a,b):\n",
    "    return a * np.exp(b*x) # \n",
    "\n",
    "# Experimental x and y\n",
    "xData = time\n",
    "yData = autocorr\n",
    "\n",
    "# Plot experimental data points\n",
    "plt.plot(xData, yData, 'bo', label='Experimental Data')\n",
    "\n",
    "# Estimate initial values\n",
    "a0 = 100\n",
    "b0 = -65\n",
    "# Perform the curve fitting\n",
    "popt, pcov = curve_fit(func, xData, yData, p0=[a0, b0])\n",
    "a, b = popt\n",
    "\n",
    "print(\"a = \", a)\n",
    "print(\"b = \", b)\n",
    "\n",
    "# xvalues of fitted function\n",
    "xFit = np.linspace(time[0], time[-1], num=1000)\n",
    "\n",
    "# Plot the fitted function\n",
    "plt.plot(xFit, func(xFit, *popt), 'r', label='Fit Data')\n",
    "\n",
    "plt.xlabel('Time lag')\n",
    "plt.ylabel('Auto correlation')\n",
    "plt.xscale(\"log\")\n",
    "plt.legend()\n",
    "plt.savefig(\"/Users/ankitgupta/Desktop/auto_fitted.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as m\n",
    "\n",
    "b =  -39.25480619405087\n",
    "k_b = 1.38 * (10**(-23))  # boltzman constant \n",
    "T = 296    # temperature\n",
    "n = 1.33  # refrective index of the water \n",
    "pi = 3.14\n",
    "theta = 13 #degree\n",
    "lemda = 532 * (10**(-9))\n",
    "eta = 10**(-3)\n",
    "import math as m\n",
    "\n",
    "K = (4*pi*n*(m.sin(13*m.pi/180)))/(lemda)\n",
    "\n",
    "print(\"K = \",K)\n",
    "\n",
    "D = -b/(2 * (K**2))\n",
    "print(\"Diffusion Constant = \",D)\n",
    "d = (k_b * T)/(3*pi*eta*D)\n",
    "print(\"Particle size = \", d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation function\n",
    "correlation = 1 - autocorr\n",
    "\n",
    "# Calculate the intensity distribution\n",
    "intensity_dist = correlation - 1\n",
    "\n",
    "# Normalize the intensity distribution\n",
    "normalized_intensity_dist = intensity_dist / np.sum(intensity_dist)\n",
    "\n",
    "# Plot the intensity distribution with thinner lines\n",
    "plt.plot(xData, normalized_intensity_dist, 'b-', linewidth=1, label='Intensity Distribution')\n",
    "plt.xlabel('Particle Size')\n",
    "plt.ylabel('Normalized Intensity')\n",
    "plt.xlim(0.2,6)\n",
    "plt.ylim(-0.0001,0.0001)\n",
    "plt.legend()\n",
    "plt.savefig(\"/Users/ankitgupta/Desktop/particle_distribution.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Calculate the mean particle size\n",
    "mean_size = np.sum(xData * normalized_intensity_dist)\n",
    "\n",
    "# Calculate the standard deviation\n",
    "variance = np.sum(((xData - mean_size) ** 2) * normalized_intensity_dist)\n",
    "std_deviation = np.sqrt(variance)\n",
    "\n",
    "print(\"Mean Particle Size:\", mean_size)\n",
    "print(\"Standard Deviation:\", std_deviation)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # for 1 micron\n",
    "\n",
    "Mean Particle Size: 0.9988221844499751 and \n",
    "Standard Deviation: 1.577473946597965"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
